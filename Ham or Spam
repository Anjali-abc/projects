import pandas as pd #read csv
import numpy as np 
import matplotlib.pyplot as plt #just plot
import seaborn as sns
import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import Model
from keras.layers import LSTM, Dense, Activation, Input, Embedding, Dropout
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical, pad_sequences
from keras.callbacks import EarlyStopping

df=pd.read_csv('md.csv')
df=df.dropna()
df.head(5)

df.Category.value_counts().plot.bar()

X=df.Code
Y=df.Category
le=LabelEncoder() #call LabelEncoder and name it le
Y=le.fit_transform(Y)
print(X)
print(Y)

Y=Y.reshape(-1,1)
print(Y)

trainx, testx, trainy, testy= train_test_split(X, Y, test_size=0.15)
#trainx-traning data for message 
#trainy-training data for category
print(trainx)
print(len(trainx))

max_words=500
max_len=2100 #our preferred size of vector/word
tk=Tokenizer(num_words=max_words) #called tokenizer and named it tk
tk.fit_on_texts(trainx) #trainx cuz its the message that needs to be broken down into words so it can be embedded
words=tk.texts_to_sequences(trainx)
print(words)
print(len(words))
#to make all vectors of same size -padding

pwords=keras.utils.pad_sequences(words, maxlen=max_len)
print(pwords)

#creating model using functional API
inputs=Input(shape=[max_len]) #input layer
layer=Embedding(max_words, 50, input_length=max_len)(inputs)#Embedding Layer
layer=LSTM(64)(layer)
layer=Dense(256)(layer)
layer=Activation('relu')(layer)
layer=Dropout(0.5)(layer)
layer=Dense(1)(layer)
layer=Activation('sigmoid')(layer)
model=Model(inputs=inputs, outputs=layer)

model.summary()

model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])

#fitting our training data on the model created
#pwords-trainx after padding
history=model.fit(pwords, trainy, batch_size=128, epochs=10,validation_split=0.3)

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], 'r', label='Training Accuracy')
plt.plot(history.history['val_accuracy'], 'g', label='Validation Accuracy')
plt.title('Training VS Validation Accuracy')
plt.xlabel('No. of Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], 'r', label='Training Loss')
plt.plot(history.history['val_loss'], 'g', label='Validation Loss')
plt.title('Training VS Validation Loss')
plt.xlabel('No. of Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

result=tk.texts_to_sequences(testx)
presult=keras.utils.pad_sequences(result, maxlen=max_len)
print(presult)
accr=model.evaluate(presult,testy)
#loss
print(accr[0])
#accuracy
print(accr[1])

